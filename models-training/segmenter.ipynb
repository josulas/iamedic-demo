{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetal Nuchal Translucency Segmenter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-20T16:42:13.899750Z",
     "iopub.status.busy": "2025-06-20T16:42:13.899141Z",
     "iopub.status.idle": "2025-06-20T16:42:23.973600Z",
     "shell.execute_reply": "2025-06-20T16:42:23.973062Z",
     "shell.execute_reply.started": "2025-06-20T16:42:13.899726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "from segmentation_models_pytorch.utils.metrics import IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "TARGET_SIZE = (400, 600)  # target size for images and masks\n",
    "MLFLOW_URI = \"http://localhost:8080\"\n",
    "BATCH_SIZE = 4 \n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"best_nt_segmenter.pth\")\n",
    "ONNX_SAVE_DIR = \"onnx_exports\"\n",
    "MODEL_SAVE_PATH = os.path.join(ONNX_SAVE_DIR, \"nt_segmenter.onnx\")\n",
    "RUN_NAME = f\"{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging with tensorboard and mlflow\n",
    "tb_writer = SummaryWriter(log_dir='tb_logs/nt_segmenter')\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(\"nt_segmenter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:42:23.975527Z",
     "iopub.status.busy": "2025-06-20T16:42:23.975137Z",
     "iopub.status.idle": "2025-06-20T16:42:23.979270Z",
     "shell.execute_reply": "2025-06-20T16:42:23.978682Z",
     "shell.execute_reply.started": "2025-06-20T16:42:23.975509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_DIR = 'preprocessed_images'\n",
    "MASK_DIR = \"segmentations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# check if windows is being used and try to import torch-directml\n",
    "elif os.name == 'nt':\n",
    "    try:\n",
    "        import torch_directml\n",
    "        device = torch_directml.device()\n",
    "    except ImportError:\n",
    "        pass\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:42:23.980359Z",
     "iopub.status.busy": "2025-06-20T16:42:23.980059Z",
     "iopub.status.idle": "2025-06-20T16:42:25.342084Z",
     "shell.execute_reply": "2025-06-20T16:42:25.341506Z",
     "shell.execute_reply.started": "2025-06-20T16:42:23.980334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Gather and filter filenames\n",
    "all_imgs = [f for f in os.listdir(IMG_DIR) if f.endswith('.png')]\n",
    "valid_imgs = [\n",
    "    img for img in all_imgs\n",
    "    if os.path.exists(os.path.join(MASK_DIR, f\"seg_{os.path.splitext(img)[0]}.png\"))\n",
    "]\n",
    "\n",
    "# Split into train / test (80/20)\n",
    "train_files, test_files = train_test_split(\n",
    "    valid_imgs,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Build full paths lists\n",
    "train_imgs  = [os.path.join(IMG_DIR,  img) for img in train_files]\n",
    "train_masks = [os.path.join(MASK_DIR, f\"seg_{os.path.splitext(img)[0]}.png\")\n",
    "               for img in train_files]\n",
    "\n",
    "test_imgs   = [os.path.join(IMG_DIR,  img) for img in test_files]\n",
    "test_masks  = [os.path.join(MASK_DIR, f\"seg_{os.path.splitext(img)[0]}.png\")\n",
    "               for img in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:42:25.348660Z",
     "iopub.status.busy": "2025-06-20T16:42:25.348394Z",
     "iopub.status.idle": "2025-06-20T16:42:25.901248Z",
     "shell.execute_reply": "2025-06-20T16:42:25.900561Z",
     "shell.execute_reply.started": "2025-06-20T16:42:25.348620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert to lists if they’re Paths\n",
    "train_files = [f.name if hasattr(f, 'name') else f for f in train_files]\n",
    "test_files  = [f.name if hasattr(f, 'name') else f for f in test_files]\n",
    "\n",
    "# Choose random samples\n",
    "train_sample = random.choice(train_files)\n",
    "test_sample  = random.choice(test_files)\n",
    "\n",
    "# Paths\n",
    "train_img  = os.path.join(IMG_DIR, train_sample)\n",
    "train_mask = os.path.join(MASK_DIR, f\"seg_{os.path.splitext(train_sample)[0]}.png\")\n",
    "test_img   = os.path.join(IMG_DIR, test_sample)\n",
    "test_mask  = os.path.join(MASK_DIR,  f\"seg_{os.path.splitext(test_sample)[0]}.png\")\n",
    "\n",
    "def plot_pair(img_path, mask_path, title):\n",
    "    img  = mpimg.imread(img_path)\n",
    "    mask = mpimg.imread(mask_path)\n",
    "    if mask.max() <= 1.0:\n",
    "        mask = mask * 255\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.set_title(f\"{title} Image: {os.path.basename(img_path)}\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(mask, cmap='Reds')\n",
    "    ax2.set_title(f\"{title} Mask: {os.path.basename(mask_path)}\")\n",
    "    ax2.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display\n",
    "plot_pair(train_img, train_mask, 'Train')\n",
    "plot_pair(test_img, test_mask,   'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:42:25.902358Z",
     "iopub.status.busy": "2025-06-20T16:42:25.902089Z",
     "iopub.status.idle": "2025-06-20T16:42:25.906852Z",
     "shell.execute_reply": "2025-06-20T16:42:25.906146Z",
     "shell.execute_reply.started": "2025-06-20T16:42:25.902334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of train images: {len(train_imgs)}\")\n",
    "print(f\"Number of train masks:  {len(train_masks)}\")\n",
    "\n",
    "print(f\"Number of test images:  {len(test_imgs)}\")\n",
    "print(f\"Number of test masks:   {len(test_masks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapes(paths):\n",
    "    shapes = [cv2.imread(p, cv2.IMREAD_UNCHANGED).shape[:2] for p in paths]\n",
    "    return Counter(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:49:31.169233Z",
     "iopub.status.busy": "2025-06-20T16:49:31.168509Z",
     "iopub.status.idle": "2025-06-20T16:49:46.832873Z",
     "shell.execute_reply": "2025-06-20T16:49:46.832221Z",
     "shell.execute_reply.started": "2025-06-20T16:49:31.169209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of train images: {len(train_imgs)}\")\n",
    "print(f\"Train image dimensions (H×W) frequency:\")\n",
    "for (h, w), cnt in get_shapes(train_imgs).items():\n",
    "    print(f\"  {h}×{w}: {cnt}\")\n",
    "\n",
    "print(f\"\\nNumber of train masks: {len(train_masks)}\")\n",
    "print(f\"Train mask dimensions (H×W) frequency:\")\n",
    "for (h, w), cnt in get_shapes(train_masks).items():\n",
    "    print(f\"  {h}×{w}: {cnt}\")\n",
    "\n",
    "print(f\"\\nNumber of test images: {len(test_imgs)}\")\n",
    "print(f\"Test image dimensions (H×W) frequency:\")\n",
    "for (h, w), cnt in get_shapes(test_imgs).items():\n",
    "    print(f\"  {h}×{w}: {cnt}\")\n",
    "\n",
    "print(f\"\\nNumber of test masks: {len(test_masks)}\")\n",
    "print(f\"Test mask dimensions (H×W) frequency:\")\n",
    "for (h, w), cnt in get_shapes(test_masks).items():\n",
    "    print(f\"  {h}×{w}: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetusSegDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None):\n",
    "        assert len(image_paths) == len(mask_paths), \"Images and masks counts must match\"\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "        self.transforms  = transforms or T.Compose([\n",
    "            T.Resize((256,256)),      # optional: down/up-sample\n",
    "            T.ToTensor(),             # [0–255] → [0–1], shape [C,H,W]\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load grayscale image & mask\n",
    "        img  = Image.open(self.image_paths[idx]).convert('L')\n",
    "        msk  = Image.open(self.mask_paths[idx]).convert('L')\n",
    "        \n",
    "        # Apply identical transforms\n",
    "        img_t = self.transforms(img)\n",
    "        msk_t = self.transforms(msk)\n",
    "        \n",
    "        # Optionally: binarize mask (if not already 0/1)\n",
    "        msk_t = (msk_t > 0.5).float()\n",
    "        \n",
    "        return img_t, msk_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:51:51.630968Z",
     "iopub.status.busy": "2025-06-20T16:51:51.630691Z",
     "iopub.status.idle": "2025-06-20T16:51:52.197969Z",
     "shell.execute_reply": "2025-06-20T16:51:52.197123Z",
     "shell.execute_reply.started": "2025-06-20T16:51:51.630949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.Resize(TARGET_SIZE),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# 1) Create datasets\n",
    "train_ds = FetusSegDataset(train_imgs,  train_masks, transforms=transforms)\n",
    "test_ds  = FetusSegDataset(test_imgs,   test_masks,  transforms=transforms)\n",
    "\n",
    "# 2) Create loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=(False if device == 'cpu' else True),\n",
    "    num_workers=(0 if os.name == 'nt' else 4)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=(False if device == 'cpu' else True),\n",
    "    num_workers=(0 if os.name == 'nt' else 4)\n",
    ")\n",
    "\n",
    "# 3) Quick check\n",
    "imgs, masks = next(iter(train_loader))\n",
    "print(\"Images batch:\", imgs.shape)   # e.g. [8, 1, 450, 600]\n",
    "print(\"Masks batch: \", masks.shape)  # e.g. [8, 1, 450, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values\n",
    "print(\"Max value in images batch:\", imgs.max().item())  # should be ~1.0\n",
    "print(\"Min value in images batch:\", imgs.min().item())  # should be ~0.0\n",
    "print(\"Max value in masks batch:\", masks.max().item())  # should be 1\n",
    "print(\"Min value in masks batch:\", masks.min().item())  # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Building blocks\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(conv ⇒ ReLU ⇒ conv ⇒ ReLU)\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,  out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.pool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super().__init__()\n",
    "        # choose upsampling method\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            mid_ch = in_ch // 2\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
    "            mid_ch = in_ch // 2\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # x1 is decoder input, x2 is encoder skip connection\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW, adjust pad if needed\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX//2, diffX - diffX//2,\n",
    "                        diffY//2, diffY - diffY//2])\n",
    "        # concatenate along channel dimension\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"1×1 conv to map to output channels\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# 2) UNet assembly\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear     = bilinear\n",
    "\n",
    "        # encoder (contracting path)\n",
    "        self.inc    = DoubleConv(in_channels, 64)\n",
    "        self.down1  = Down(64, 128)\n",
    "        self.down2  = Down(128, 256)\n",
    "        self.down3  = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4  = Down(512, 1024 // factor)\n",
    "\n",
    "        # decoder (expansive path)\n",
    "        self.up1    = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2    = Up(512, 256 // factor, bilinear)\n",
    "        self.up3    = Up(256, 128 // factor, bilinear)\n",
    "        self.up4    = Up(128, 64, bilinear)\n",
    "\n",
    "        # final 1x1 convolution\n",
    "        self.outc   = OutConv(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x1 = self.inc(x)      # 64 channels\n",
    "        x2 = self.down1(x1)   # 128\n",
    "        x3 = self.down2(x2)   # 256\n",
    "        x4 = self.down3(x3)   # 512\n",
    "        x5 = self.down4(x4)   # 1024\n",
    "\n",
    "        # decoding with skip connections\n",
    "        x  = self.up1(x5, x4)\n",
    "        x  = self.up2(x,  x3)\n",
    "        x  = self.up3(x,  x2)\n",
    "        x  = self.up4(x,  x1)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = UNet(in_channels=1, out_channels=1, bilinear=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary segmentation: logits → mask\n",
    "loss_fn   = nn.BCEWithLogitsLoss()\n",
    "metric_fn = IoU(threshold=0.5) \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values for early stopping\n",
    "best_val_iou = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME):\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"initial_learning_rate\": LEARNING_RATE,\n",
    "        \"model_type\": \"FetusNTSegmenter\",\n",
    "        \"backbone\": \"UNet\"\n",
    "    })\n",
    "    for epoch in tqdm(range(1, NUM_EPOCHS + 1), desc=\"Epochs\", position=0, leave=True):\n",
    "        # Train\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=\"Training Batches\", position=1, leave=False)\n",
    "        model.train()\n",
    "        for imgs, masks in train_pbar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs)\n",
    "            loss   = loss_fn(logits, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        train_pbar.close()\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        tb_writer.add_scalar('Loss/train', epoch_train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_iou = 0.0, 0.0\n",
    "        val_pbar = tqdm(test_loader, desc=f'Validation Batches', position=2, leave=False)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_pbar:\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                logits = model(imgs)\n",
    "                loss = loss_fn(logits, masks)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                preds = torch.sigmoid(logits)\n",
    "                val_iou += metric_fn(preds, masks).item() * imgs.size(0)\n",
    "        val_pbar.close()\n",
    "        epoch_val_loss = val_loss / len(test_loader.dataset)\n",
    "        epoch_val_iou  = val_iou  / len(test_loader.dataset)\n",
    "        tb_writer.add_scalar('Loss/val', epoch_val_loss, epoch)\n",
    "        tb_writer.add_scalar('IoU/val', epoch_val_iou, epoch)\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": epoch_train_loss,\n",
    "            \"val_loss\": epoch_val_loss,\n",
    "            \"val_iou\": epoch_val_iou,\n",
    "            \"current_learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        }, step=epoch)\n",
    "\n",
    "        # Early stopping and checkpoint\n",
    "        if epoch_val_iou > best_val_iou:\n",
    "            best_val_iou = epoch_val_iou\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best val IoU: {best_val_iou:.4f}\")\n",
    "                break\n",
    "    # Close TensorBoard writer\n",
    "    tb_writer.close()\n",
    "    \n",
    "    # Load best weights at the end\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "\n",
    "    # Save the final model\n",
    "    if not os.path.exists(ONNX_SAVE_DIR):\n",
    "        os.makedirs(ONNX_SAVE_DIR)\n",
    "    dummy_input = torch.randn(1, 1, TARGET_SIZE[0], TARGET_SIZE[1], device=device)\n",
    "    torch.onnx.export(\n",
    "        model, \n",
    "        dummy_input, \n",
    "        MODEL_SAVE_PATH, \n",
    "        input_names=['input'], \n",
    "        output_names=['output'],\n",
    "        opset_version=11,\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    mlflow.log_artifact(MODEL_SAVE_PATH)\n",
    "    mlflow.log_metrics({\n",
    "        \"final_val_iou\": best_val_iou\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Single-image inference example\n",
    "img, _ = next(iter(test_loader))\n",
    "img = img.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(img)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).float()\n",
    "\n",
    "# Visualize first sample\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img[0,0].cpu(), cmap='gray')\n",
    "plt.title(\"Input\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(preds[0,0].cpu(), cmap='Reds')\n",
    "plt.title(\"Pred Mask\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(img[0,0].cpu(), cmap='gray'); \n",
    "plt.imshow(preds[0,0].cpu(), cmap='Reds', alpha=0.3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6244612,
     "sourceId": 10133802,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "iamedic-data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
