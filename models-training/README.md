# Models Training

This directory contains the code and resources for training two deep learning models for fetal ultrasound image analysis:
1. **Anatomy Detector** (localizer.ipynb): A model that detects and localizes different fetal anatomical structures in ultrasound images
2. **NT Segmenter** (segmenter.ipynb): A model that segments the Nuchal Translucency (NT) region in fetal ultrasound images

## Directory Structure

- **dataset/**: Contains the original ultrasound images and an Excel file with bounding box annotations
  - `ObjectDetection.xlsx`: Excel file with bounding box annotations for different anatomical structures
  - `Dataset for Fetus Framework/`: Original ultrasound images organized in subdirectories

- **preprocessed_images/**: Contains preprocessed ultrasound images used for training the segmentation model

- **segmentations/**: Contains binary masks for the NT region, used for training the segmentation model
  - Each mask is named `seg_X.png` where X corresponds to the filename of the image in preprocessed_images/

- **training_set/**: Contains images that have at least one bounding box annotation, generated by the EDA notebook
  - Used for training the anatomy detector model

- **checkpoints/**: Stores the best model weights during training
  - `best_localizer.pth`: Best weights for the anatomy detector model
  - `best_nt_segmenter.pth`: Best weights for the NT segmenter model

- **onnx_exports/**: Contains exported ONNX models for deployment
  - `anatomy_detector.onnx`: Exported anatomy detector model
  - `nt_segmenter.onnx`: Exported NT segmenter model

- **tb_logs/**: TensorBoard logs for monitoring training progress
  - `anatomy_detector/`: Logs for the anatomy detector model
  - `nt_segmenter/`: Logs for the NT segmenter model

- **mlruns/**: MLflow tracking data for experiment management

## Workflow

The training workflow consists of the following steps:

1. **Data Preparation (EDA.ipynb)**:
   - Reads the `ObjectDetection.xlsx` file containing bounding box annotations
   - Analyzes the dataset structure and visualizes the frequency of different anatomical structures
   - Creates a `training_set` directory containing only images that have at least one bounding box annotation
   - This step must be run first to generate the training set for the localizer model

2. **Anatomy Detector Training (localizer.ipynb)**:
   - Uses the `training_set` directory and `ObjectDetection.xlsx` for training data
   - Implements a MobileNetV2-based model for detecting and localizing fetal anatomical structures
   - Trains the model with early stopping based on validation mAP
   - Exports the trained model to ONNX format for deployment

3. **NT Segmenter Training (segmenter.ipynb)**:
   - Uses the `preprocessed_images` directory for input images and `segmentations` directory for ground truth masks
   - Implements a U-Net architecture for segmenting the NT region
   - Trains the model with early stopping based on validation IoU
   - Exports the trained model to ONNX format for deployment

## Models

### Anatomy Detector
- **Architecture**: MobileNetV2 backbone with custom detection head
- **Input**: Grayscale ultrasound images (1 channel)
- **Output**: 
  - Class probabilities for 9 different anatomical structures
  - Bounding box coordinates (center_x, center_y, width, height) for each structure
- **Metrics**: Mean Average Precision (mAP) at IoU threshold of 0.5

### NT Segmenter
- **Architecture**: U-Net
- **Input**: Grayscale ultrasound images (1 channel)
- **Output**: Binary segmentation mask for the NT region
- **Metrics**: Intersection over Union (IoU)

## Usage

1. Run the EDA notebook first to generate the training set:
   ```
   jupyter notebook EDA.ipynb
   ```

2. Train the anatomy detector model:
   ```
   jupyter notebook localizer.ipynb
   ```

3. Train the NT segmenter model:
   ```
   jupyter notebook segmenter.ipynb
   ```

4. For inference, use the corresponding inference notebooks:
   - `localizer-inference.ipynb`: For testing the anatomy detector model
   - `segmenter-inference.ipynb`: For testing the NT segmenter model

## Requirements

The required packages are listed in `requirements.txt`. Install them using:
```
pip install -r requirements.txt
```

## Experiment Tracking

Both training notebooks use MLflow for experiment tracking and TensorBoard for visualization. To view the training progress:

1. TensorBoard:
   ```
   tensorboard --logdir=tb_logs
   ```

2. MLflow UI (if running locally):
   ```
   mlflow ui --port 8080
   ```